---
layout: post
title: 大模型学习笔记-基于《Deep Dive into LLMs like ChatGPT》
date: 2025-03-02
---

模型是什么
从逻辑层面理解，模型类似于函数。给定模型一个输入，它会产生一个输出。
例如，垃圾邮件识别器就是一种模型。当输入一封电子邮件时，它会输出该邮件是否为垃圾邮件的判断结果。天气预测同样是一个模型，输入历史天气数据，它会输出对明天天气情况的预测。
这些都是模型的实例。当然，模型的输出存在一定的准确率，其结果可能与预期高度相符，也可能偏离较大。
构建数据集
对于大语言模型（LLM）的训练而言，首先需要构建基础数据，即收集人类知识。这一过程相当于使用爬虫程序抓取整个互联网的内容。在此，推荐使用数据集 fineweb： HuggingFaceFW/fineweb · Datasets at Hugging Face
需要注意的是，抓取的网页数据需要进行清洗。我们期望 AI 学习的内容不包含色情、暴力、种族歧视等有害信息，以及个人身份信息（PII）。
完成内容抓取后，我们会得到一个包含大量信息的“大文档”。接下来需要对其进行 token 化处理。
Token 可翻译为语素，可理解为将自然语言拆分后的基本单位，便于计算机进行处理。需要明确的是，token 并不等同于单词。
对于英语，由于单词之间天然存在空格，便于分割成 token。但对于中文这种没有空格分隔的语言，准确切分则存在一定难度。例如，“我刚才吃了西瓜籽”就有多种切分方式。
以下展示了不同模型的切分情况：
 deepseek - R1  
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/7e4997e453d5483781e48fd1e79fc4df.png)
  qwen 

![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/6e52c263a4ba4aec9edcbe874ca6c052.png)

不同的模型可能会使用不同的 tokenizer。可以通过 Tiktokenizer 快速查看一句话在不同模型下 token 化后的结果： Tiktokenizer
Token 化之后，相当于将“大文档”切分成一个个 token。模型实际处理的是 token 化后的内容，而非原始的完整句子。
实际上，还需要进行 embedding 操作，即将 token 映射到对应的向量空间，简单来说，就是将 token 转换为可用于计算的形式，至此便可以开始模型训练。不过，相关视频并未提及这一点。从整体理解的角度出发，在模型训练过程中，需要重点关注的是模型“看到”的是 token，而非完整的单词或句子。
预训练（Pre - train）
预训练通常是我们所理解的“训练”过程，这是整个训练流程中最耗费时间和算力的阶段，其目的是让模型学习互联网上的海量知识。
我们常常会看到在模型名称后面带有“7B”之类的描述，这表示该模型拥有 70 亿个参数。可以将模型想象成一台巨大的机器，上面有 70 亿个调节旋钮，我们需要将每个旋钮调整到合适的位置，以获得最准确的输出结果。而调整这些参数的过程就是预训练。
以最简单的一元线性回归模型为例，这也是机器学习的一种方式，通过这个过程可以直观感受模型训练的概念。然而，LLM 的模型结构远比线性回归模型复杂。可以通过以下链接查看可视化后的模型结构： https://bbycroft.net/llm
因此，找到最合适的参数需要大量的算力和时间。
预训练的结果是得到一个基座模型（Base Model）。但这个基座模型距离我们日常使用的 ChatGPT 还有很大差距。当前的基座模型更像是一个复读机，它能够根据输入预测后续的输出，但看起来并不智能，只是像背诵课文一样输出所学到的知识。
因此，需要对基座模型进行进一步训练，使其成为能够进行聊天对话的聊天机器人。
后训练（Post - train，SFT - Supervised Fine - Tuning，有监督微调）
我们期望模型成为一个能够友好对话的聊天机器人，使用平和友好的语气交流，更像是人类的助手，而非复读机。
为了实现这一目标，需要进行后训练，即 SFT。与预训练相比，后训练所使用的数据集规模较小，所需的算力和时间也相对较少。
作为对话的开源数据集，可以参考以下链接： https://huggingface.co/datasets/OpenAssistant/oasst2/viewer/default/train?p=2&views[]=train
示例数据集如下所示： 
![在这里插入图片描述](https://i-blog.csdnimg.cn/direct/e4d62744a78840eeabf83c296f9014e6.png)

需要注意的是，大模型常见的“幻觉”问题，即模型一本正经地胡说八道，也可以通过 SFT 进行改善。当我们向大模型询问一个不存在的人物信息时，模型可能会随意编造内容。这是因为在预训练过程中，模型仅学习了已有的知识，对于不存在的信息，由于互联网上没有相关内容，模型并不知晓自己的知识局限，只能从已知知识中猜测回复。
而通过 SFT，可以让模型学会在遇到不知道的问题时，回答“我不知道”，这是预训练无法解决的问题。
强化学习（Reinforcement Learning）
经过后训练，我们已经得到了类似 ChatGPT 的模型，但仍有进一步优化的空间。春节期间爆火的 DeepSeek 表现出色，其秘诀在于强化学习（RL）。
我们可以将模型的学习过程类比为学生学习数学或物理的过程。在学习时，课本会先提供相关的背景知识，然后给出例题，展示解决问题的完整步骤，最后会有大量的练习题，并附有答案和解题步骤。
学生首先学习背景知识，然后通过例题了解解题方法，接着通过做大量练习题，对照答案来检验自己对知识的理解程度，在这个过程中逐渐掌握解题思路。需要强调的是，这里的解题思路不同于解题步骤，解题步骤通常较为固定，而解题思路是每个人独特的思考方式，难以用言语清晰表达，往往是在大量练习中获得的“灵感”。
对应到 LLM 训练中，预训练相当于学习背景知识，模型会接触大量内容并进行理解和记忆；后训练则类似于例题，为模型提供完整的解题步骤和思路，让模型按照特定格式进行学习。但此时模型尚未形成自己的解题思路。
对于一个问题到答案的解决思路有多种，人类提供给模型的是自己认为最合适的思路，但人类与模型存在较大差异，对人类而言最合适的思路未必适合模型。我们更希望模型能够找到自己最擅长的解题思路，从而提高答案的质量。
因此，通过向模型提供问题和答案，让模型自主探索，找到最适合自己的思路，这就是强化学习的过程。
哪种输出更好，该如何提示（Prompt）
假设给模型出一道数学题，并希望得到解题步骤和答案，模型可能会有两种输出方式： 第一种是先给出答案，再给出解题步骤； 第二种是先给出解题步骤，再给出答案。
哪种输出方式更好呢？原因是什么？
答案是第二种。因为第一种方式相当于模型要用一个 token 的计算力完成整个问题的解答；而第二种方式中，模型可以在前面的解题步骤中逐步思考，最后给出答案，能够利用更多 token 的计算力。显然，我们希望最终答案的计算能够使用更多的计算力，这样可以提高计算的准确性。
这也对应了提示工程（Prompt Engineering）中的思维链（CoT）方法，之前在学习提示工程时，只知道这样做效果更好，但不清楚原理，现在便明白了其中的缘由。
为什么模型数不清草莓（strawberry）中有几个 r？
对于模型而言，它“看”到的并非“strawberry”这个单词，而是一个 token，例如可能是“1289”这样的数字（实际上可能是 embedding 后的向量，但这一细节并不影响理解）。由于模型无法直接看到“strawberry”这个单词，所以它无法像人类一样轻松地数出其中“r”的个数。因此，我们应避免让模型执行这类任务。更好的方法是让模型编写代码来解决问题，然后运行代码得到结果，即让模型发挥其擅长编写代码的优势，借助编程语言的能力完成自身不擅长的任务。
为什么模型认为 9.11 大于 9.9？
与上述情况类似，模型看到的不是“9.11”和“9.9”这两个具体数字，而是对应的 token，因此很难对它们进行大小比较。同时，在相关视频中，Karpathy 介绍了一个案例，研究团队对模型进行调试，观察模型回答该问题时的过程（哪些神经元被激活），结果发现与圣经内容相关。由于圣经有众多章节，如 1.1、1.2 等，在这种体系下，9.11 可理解为第九章第十一小节，自然在 9.9（第九章第九小节）之后，所以模型得出“大于”的结论也符合其内在逻辑。
同样，我们应避免让模型执行其不擅长的任务。
模型如何回答“你是谁？”
相信很多人在与模型对话时都问过这个问题，得到的回答可能会令人惊艳。
例如，询问豆包（AI 编程助手）后，得到的答案是： “我是豆包编程助手，是一个对编程和机器学习等领域有很强专业性的智能助手。我可以为你解答编程、机器学习相关的技术问题，比如编程语言的使用、算法设计、模型训练等，还能提供代码示例、项目思路等方面的帮助。如果你有相关需求，随时都能问我。”
这看似神奇，但很有可能是在 SFT 过程中添加了这样的对话内容，或者在提示信息中已经硬编码了类似内容。
实际上，这个回答可能是人工编写的，模型只是像复读机一样根据问题输出预先设定的内容。目前来看，模型尚未像科幻电影中那样真正拥有“智能”，但这一天或许很快就会到来。
